{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Rules:\n",
      "Rule 1: l + o -> lo\n",
      "Rule 2: lo + w -> low\n",
      "Rule 3: n + e -> ne\n",
      "Rule 4: ne + w -> new\n",
      "Rule 5: e + r -> er\n",
      "[('l', 'o'), ('lo', 'w'), ('n', 'e'), ('ne', 'w'), ('e', 'r')]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from collections import defaultdict \n",
    "\n",
    "def get_stat_pairs(vocab): \n",
    "    pairs = defaultdict(int) \n",
    "    for word, freq in vocab.items(): \n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1): \n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs \n",
    "\n",
    "def merge_freq_vocab(pair, v_in): \n",
    "    v_out = {} \n",
    "    bigram = re.escape(' '.join(pair)) \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in v_in: \n",
    "        w_out = p.sub(''.join(pair), word) \n",
    "        v_out[w_out] = v_in[word] \n",
    "    return v_out \n",
    "\n",
    "def get_initial_vocab(data): \n",
    "    vocab = defaultdict(int) \n",
    "    for line in data: \n",
    "        for word in line.split(): \n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab \n",
    "\n",
    "def byte_pair_encoding(data, n): \n",
    "    vocab = get_initial_vocab(data) \n",
    "    rules = []\n",
    "    for i in range(n): \n",
    "        pairs = get_stat_pairs(vocab) \n",
    "        best = max(pairs, key=pairs.get) \n",
    "        rules.append(best)\n",
    "        vocab = merge_freq_vocab(best, vocab) \n",
    "    return vocab, rules\n",
    "\n",
    "training_sentences = \"low lowest newer wider new\"\n",
    "training_data = training_sentences.split()\n",
    "num_iterations = 5\n",
    "\n",
    "vocab, rules = byte_pair_encoding(training_data, num_iterations)\n",
    "print(\"Generated Rules:\")\n",
    "for i, rule in enumerate(rules, start=1):\n",
    "    print(f\"Rule {i}: {rule[0]} + {rule[1]} -> {rule[0]}{rule[1]}\")\n",
    "\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Sentence: lower\n",
      "List of Subword Tokens: ['l', 'o', 'w', 'e', 'r', '</w>']\n",
      "\n",
      "Applying Rule: l + o -> lo\n",
      "Merged Tokens: ['lo', 'w', 'e', 'r', '</w>']\n",
      "\n",
      "Applying Rule: lo + w -> low\n",
      "Merged Tokens: ['low', 'e', 'r', '</w>']\n",
      "\n",
      "Applying Rule: n + e -> ne\n",
      "Merged Tokens: ['low', 'e', 'r', '</w>']\n",
      "\n",
      "Applying Rule: ne + w -> new\n",
      "Merged Tokens: ['low', 'e', 'r', '</w>']\n",
      "\n",
      "Applying Rule: e + r -> er\n",
      "Merged Tokens: ['low', 'er', '</w>']\n",
      "\n",
      "Final List of Subword Tokens: ['low', 'er', '</w>']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"lower\"\n",
    "words = test_sentence.split(' ')\n",
    "\n",
    "tokens = []\n",
    "for word in words:\n",
    "    tokens += [char for char in list(word) + ['</w>']]\n",
    "\n",
    "print(\"Original Test Sentence:\", test_sentence)\n",
    "print(\"List of Subword Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "def apply_rules(rules,tokens):\n",
    "    merged_tokens = tokens\n",
    "    for rule in rules:\n",
    "        for i in range(len(merged_tokens)-1):\n",
    "            if merged_tokens[i] == rule[0] and merged_tokens[i+1] == rule[1]:\n",
    "                merged_tokens[i] = rule[0] + rule[1]\n",
    "                merged_tokens[i+1] = \"\"\n",
    "        # Removing the empty tokens\n",
    "        merged_tokens = [token for token in merged_tokens if token != \"\"]\n",
    "        print(f\"Applying Rule: {rule[0]} + {rule[1]} -> {rule[0]}{rule[1]}\")\n",
    "        print(\"Merged Tokens:\",merged_tokens)\n",
    "        print()\n",
    "    return merged_tokens\n",
    "\n",
    "print(\"Final List of Subword Tokens:\", apply_rules(rules, tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
